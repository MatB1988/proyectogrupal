{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este etl tiene como objetivo poder disponer de toda la informacion de Yelp recibida en formato parquet para ser procesada con pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import fastparquet as fp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el repositorio local, este archivo etl_yelp.ipynb, toma los datos del dataset consigna de una carpeta llamada Yelp, en la que se encuentran los archivos:\n",
    "business.pkl\n",
    "checkin.json\n",
    "review.json\n",
    "tip.json\n",
    "user.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "business.pkl to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta a la carpeta Yelp\n",
    "yelp_folder = 'Yelp'\n",
    "\n",
    "file_to_df = 'business.pkl'\n",
    "# Leer el archivo business.pkl\n",
    "business_df = pd.read_pickle(os.path.join(yelp_folder, file_to_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 150346 entries, 0 to 150345\n",
      "Data columns (total 28 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   business_id   150346 non-null  object\n",
      " 1   name          150346 non-null  object\n",
      " 2   address       150346 non-null  object\n",
      " 3   city          150346 non-null  object\n",
      " 4   state         150343 non-null  object\n",
      " 5   postal_code   150346 non-null  object\n",
      " 6   latitude      150346 non-null  object\n",
      " 7   longitude     150346 non-null  object\n",
      " 8   stars         150346 non-null  object\n",
      " 9   review_count  150346 non-null  object\n",
      " 10  is_open       150346 non-null  object\n",
      " 11  attributes    136602 non-null  object\n",
      " 12  categories    150243 non-null  object\n",
      " 13  hours         127123 non-null  object\n",
      " 14  business_id   5 non-null       object\n",
      " 15  name          5 non-null       object\n",
      " 16  address       5 non-null       object\n",
      " 17  city          5 non-null       object\n",
      " 18  state         5 non-null       object\n",
      " 19  postal_code   5 non-null       object\n",
      " 20  latitude      5 non-null       object\n",
      " 21  longitude     5 non-null       object\n",
      " 22  stars         5 non-null       object\n",
      " 23  review_count  5 non-null       object\n",
      " 24  is_open       5 non-null       object\n",
      " 25  attributes    5 non-null       object\n",
      " 26  categories    5 non-null       object\n",
      " 27  hours         5 non-null       object\n",
      "dtypes: object(28)\n",
      "memory usage: 33.3+ MB\n"
     ]
    }
   ],
   "source": [
    "business_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un nuevo DataFrame con las primeras 14 columnas\n",
    "df_business = business_df.iloc[:, :14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 150346 entries, 0 to 150345\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   business_id   150346 non-null  object\n",
      " 1   name          150346 non-null  object\n",
      " 2   address       150346 non-null  object\n",
      " 3   city          150346 non-null  object\n",
      " 4   state         150343 non-null  object\n",
      " 5   postal_code   150346 non-null  object\n",
      " 6   latitude      150346 non-null  object\n",
      " 7   longitude     150346 non-null  object\n",
      " 8   stars         150346 non-null  object\n",
      " 9   review_count  150346 non-null  object\n",
      " 10  is_open       150346 non-null  object\n",
      " 11  attributes    136602 non-null  object\n",
      " 12  categories    150243 non-null  object\n",
      " 13  hours         127123 non-null  object\n",
      "dtypes: object(14)\n",
      "memory usage: 17.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_business.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame guardado como parquet\\business.parquet\n"
     ]
    }
   ],
   "source": [
    "# Verificar si el directorio 'parquet' existe, y si no, crearlo\n",
    "if not os.path.exists('parquet'):\n",
    "    os.makedirs('parquet')\n",
    "\n",
    "# Obtener el nombre del archivo original sin la extensión\n",
    "df_2_file_name = 'business.pkl'.split('.')[0]\n",
    "\n",
    "# Guardar el DataFrame en formato Parquet en el directorio 'parquet' con el nombre del archivo original\n",
    "file_path = os.path.join('parquet', f'{df_2_file_name}.parquet')\n",
    "\n",
    "# Guardar el DataFrame en formato Parquet usando fastparquet\n",
    "df_business.to_parquet(file_path, engine='fastparquet')\n",
    "\n",
    "print(f\"DataFrame guardado como {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkin.json to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo checkin.json dentro de la carpeta Yelp\n",
    "checkin_path = os.path.join('Yelp', 'checkin.json')\n",
    "\n",
    "# Lista para almacenar los DataFrames\n",
    "checkin_dfs = []\n",
    "\n",
    "# Leer el archivo línea por línea y convertir cada línea en un DataFrame\n",
    "with open(checkin_path, 'r') as checkin_file:\n",
    "    for line in checkin_file:\n",
    "        data = eval(line)  # Convertir la línea en un diccionario\n",
    "        df = pd.DataFrame([data])  # Crear un DataFrame a partir del diccionario\n",
    "        checkin_dfs.append(df)\n",
    "\n",
    "# Concatenar los DataFrames en uno solo\n",
    "checkin_df = pd.concat(checkin_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 131930 entries, 0 to 131929\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   business_id  131930 non-null  object\n",
      " 1   date         131930 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "checkin_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna: business_id\n",
      "0    ---kPU91CF4Lq2-WlRu9Lw\n",
      "1    --0iUa4sNDFiZFrAdIWhZQ\n",
      "2    --30_8IhuyMHbSOcNWd6DQ\n",
      "3    --7PUidqRWpRSpXebiyxTg\n",
      "4    --7jw19RH9JKXgFohspgQw\n",
      "5    --8IbOsAAxjKRoYsBFL-PA\n",
      "6    --9osgUCSDUWUkoTLdvYhQ\n",
      "7    --ARBQr1WMsTWiwOKOj-FQ\n",
      "8    --FWWsIwxRwuw9vIMImcQg\n",
      "9    --FcbSxK1AoEtEAxOgBaCw\n",
      "Name: business_id, dtype: object\n",
      "\n",
      "\n",
      "Columna: date\n",
      "0    2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020...\n",
      "1    2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011...\n",
      "2             2013-06-14 23:29:17, 2014-08-13 23:20:22\n",
      "3    2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012...\n",
      "4    2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014...\n",
      "5    2015-06-06 01:03:19, 2015-07-29 16:50:58, 2015...\n",
      "6    2015-06-13 02:00:57, 2015-07-04 00:44:09, 2015...\n",
      "7    2014-12-12 00:44:23, 2015-01-09 00:19:52, 2015...\n",
      "8    2010-09-11 16:28:39, 2010-12-22 21:14:19, 2011...\n",
      "9    2017-08-18 19:43:50, 2017-10-07 22:38:38, 2017...\n",
      "Name: date, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los primeros 10 registros de cada campo \n",
    "for column in checkin_df.columns:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(checkin_df[column].head(10))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame guardado como parquet\\checkin.parquet\n"
     ]
    }
   ],
   "source": [
    "# Verificar si el directorio 'parquet' existe, y si no, crearlo\n",
    "if not os.path.exists('parquet'):\n",
    "    os.makedirs('parquet')\n",
    "\n",
    "# Obtener el nombre del archivo original sin la extensión\n",
    "df_2_file_name = 'checkin.json'.split('.')[0]\n",
    "\n",
    "# Guardar el DataFrame en formato Parquet en el directorio 'parquet' con el nombre del archivo original\n",
    "file_path = os.path.join('parquet', f'{df_2_file_name}.parquet')\n",
    "\n",
    "# A continuación, se guarda el DataFrame en formato Parquet usando fastparquet\n",
    "checkin_df.to_parquet(file_path, engine='fastparquet')\n",
    "\n",
    "print(f\"DataFrame guardado como {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tip.json to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo tip.json dentro de la carpeta Yelp\n",
    "tip_path = os.path.join('Yelp', 'tip.json')\n",
    "\n",
    "# Lista para almacenar los DataFrames\n",
    "tip_df = []\n",
    "\n",
    "# Leer el archivo línea por línea y convertir cada línea en un DataFrame\n",
    "with open(tip_path, 'r', encoding='utf-8') as tip_file:\n",
    "    for line in tip_file:\n",
    "        data = json.loads(line)  # Convertir la línea en un diccionario\n",
    "        df = pd.DataFrame([data])  # Crear un DataFrame a partir del diccionario\n",
    "        tip_df.append(df)\n",
    "\n",
    "# Concatenar los DataFrames en uno solo\n",
    "tip_df = pd.concat(tip_df, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 908915 entries, 0 to 908914\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   user_id           908915 non-null  object\n",
      " 1   business_id       908915 non-null  object\n",
      " 2   text              908915 non-null  object\n",
      " 3   date              908915 non-null  object\n",
      " 4   compliment_count  908915 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 34.7+ MB\n"
     ]
    }
   ],
   "source": [
    "tip_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna: user_id\n",
      "0    AGNUgVwnZUey3gcPCJ76iw\n",
      "1    NBN4MgHP9D3cw--SnauTkA\n",
      "2    -copOvldyKh1qr-vzkDEvw\n",
      "3    FjMQVZjSqY8syIO-53KFKw\n",
      "4    ld0AperBXk1h6UbqmM80zw\n",
      "5    trf3Qcz8qvCDKXiTgjUcEg\n",
      "6    SMGAlRjyfuYu-c-22zIyOg\n",
      "7    YVBB9g23nuVJ0u44zK0pSA\n",
      "8    VL12EhEdT4OWqGq0nIqkzw\n",
      "9    4ay-fdVks5WMerYL_htkGQ\n",
      "Name: user_id, dtype: object\n",
      "\n",
      "\n",
      "Columna: business_id\n",
      "0    3uLgwr0qeCNMjKenHJwPGQ\n",
      "1    QoezRbYQncpRqyrLH6Iqjg\n",
      "2    MYoRNLb5chwjQe3c_k37Gg\n",
      "3    hV-bABTK-glh5wj31ps_Jw\n",
      "4    _uN0OudeJ3Zl_tf6nxg5ww\n",
      "5    7Rm9Ba50bw23KTA8RedZYg\n",
      "6    kH-0iXqkL7b8UXNpguBMKg\n",
      "7    jtri188kuhe_AuEOJ51U_A\n",
      "8    xODBZmX4EmlVvbqtKN7YKg\n",
      "9    pICJRcyqW1cF96Q3XhLSbw\n",
      "Name: business_id, dtype: object\n",
      "\n",
      "\n",
      "Columna: text\n",
      "0                       Avengers time with the ladies.\n",
      "1    They have lots of good deserts and tasty cuban...\n",
      "2               It's open even when you think it isn't\n",
      "3                            Very decent fried chicken\n",
      "4               Appetizers.. platter special for lunch\n",
      "5    Chili Cup + Single Cheeseburger with onion, pi...\n",
      "6    Saturday, Dec 7th 2013, ride Patco's Silver Sl...\n",
      "7    This is probably the best place in the cool Sp...\n",
      "8                                                Tacos\n",
      "9    Starbucks substitute in boring downtown Tampa....\n",
      "Name: text, dtype: object\n",
      "\n",
      "\n",
      "Columna: date\n",
      "0    2012-05-18 02:17:21\n",
      "1    2013-02-05 18:35:10\n",
      "2    2013-08-18 00:56:08\n",
      "3    2017-06-27 23:05:38\n",
      "4    2012-10-06 19:43:09\n",
      "5    2012-03-13 04:00:52\n",
      "6    2013-12-03 23:42:15\n",
      "7    2016-11-22 22:14:58\n",
      "8    2012-07-27 01:48:24\n",
      "9    2012-06-09 22:57:04\n",
      "Name: date, dtype: object\n",
      "\n",
      "\n",
      "Columna: compliment_count\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "9    0\n",
      "Name: compliment_count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los primeros 10 registros de cada campo \n",
    "for column in tip_df.columns:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(tip_df[column].head(10))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538797   2022-01-19 20:38:55\n",
      "741844   2022-01-19 19:20:35\n",
      "901828   2022-01-19 19:07:52\n",
      "697249   2022-01-19 19:06:01\n",
      "536750   2022-01-19 18:46:08\n",
      "353922   2022-01-19 18:42:44\n",
      "826026   2022-01-19 17:40:43\n",
      "877776   2022-01-19 17:37:41\n",
      "505905   2022-01-19 17:33:53\n",
      "541978   2022-01-19 17:16:57\n",
      "702824   2022-01-19 16:58:06\n",
      "412609   2022-01-19 16:49:51\n",
      "539362   2022-01-19 15:48:50\n",
      "543747   2022-01-19 13:10:15\n",
      "421768   2022-01-19 12:25:12\n",
      "696565   2022-01-19 12:11:31\n",
      "529662   2022-01-19 06:33:22\n",
      "529685   2022-01-19 05:23:53\n",
      "543800   2022-01-19 04:47:00\n",
      "821171   2022-01-19 03:48:47\n",
      "Name: date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Transformar columna 'date' a formato datetime\n",
    "tip_df['date'] = pd.to_datetime(tip_df['date'])\n",
    "\n",
    "# Luego, ordena el DataFrame por la columna 'date' en orden descendente y selecciona las últimas 20 fechas.\n",
    "last_20_dates = tip_df['date'].sort_values(ascending=False).head(20)\n",
    "\n",
    "# Imprime las últimas 20 fechas\n",
    "print(last_20_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame guardado como parquet\\tip.parquet\n"
     ]
    }
   ],
   "source": [
    "# Verificar si el directorio 'parquet' existe, y si no, crearlo\n",
    "if not os.path.exists('parquet'):\n",
    "    os.makedirs('parquet')\n",
    "\n",
    "# Obtener el nombre del archivo original sin la extensión\n",
    "df_2_file_name = 'tip.json'.split('.')[0]\n",
    "\n",
    "# Guardar el DataFrame en formato Parquet en el directorio 'parquet' con el nombre del archivo original\n",
    "file_path = os.path.join('parquet', f'{df_2_file_name}.parquet')\n",
    "\n",
    "# A continuación, se guarda el DataFrame en formato Parquet usando fastparquet\n",
    "tip_df.to_parquet(file_path, engine='fastparquet')\n",
    "\n",
    "print(f\"DataFrame guardado como {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "review.json to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo original y copia\n",
    "archivo_original = 'Yelp/review.json'\n",
    "archivo_copia = 'Yelp/review_copia.json'\n",
    "\n",
    "# Copiar el archivo original a review_copia.json\n",
    "with open(archivo_original, 'r', encoding='utf-8') as original_file:\n",
    "    data = [json.loads(line) for line in original_file]\n",
    "    with open(archivo_copia, 'w', encoding='utf-8') as copia_file:\n",
    "        for line in data:\n",
    "            json.dump(line, copia_file)\n",
    "            copia_file.write('\\n')\n",
    "\n",
    "# Número de líneas por archivo parquet\n",
    "lineas_por_archivo = 1000000\n",
    "numero_archivo = 1\n",
    "linea_actual = 0\n",
    "df_actual = []\n",
    "lista_df_reviews = []  # Lista para almacenar DataFrames\n",
    "\n",
    "# Crear la carpeta para guardar los archivos parquet\n",
    "if not os.path.exists('pq_review'):\n",
    "    os.makedirs('pq_review')\n",
    "\n",
    "for linea in data:\n",
    "    df_actual.append(linea)\n",
    "    linea_actual += 1\n",
    "\n",
    "    if linea_actual == lineas_por_archivo:\n",
    "        # Crear un DataFrame\n",
    "        df = pd.DataFrame(df_actual)\n",
    "        \n",
    "        # Agregar el DataFrame a la lista\n",
    "        lista_df_reviews.append(df)\n",
    "\n",
    "        # Nombre del archivo parquet\n",
    "        nombre_archivo = f'pq_review/review_{str(numero_archivo).zfill(2)}.parquet'\n",
    "\n",
    "        # Guardar el DataFrame como archivo parquet\n",
    "        df.to_parquet(nombre_archivo, index=False)\n",
    "\n",
    "        # Reiniciar el contador de líneas y el DataFrame actual\n",
    "        linea_actual = 0\n",
    "        df_actual = []\n",
    "        numero_archivo += 1\n",
    "\n",
    "# Si quedan líneas en el último DataFrame\n",
    "if df_actual:\n",
    "    df = pd.DataFrame(df_actual)\n",
    "    \n",
    "    # Agregar el último DataFrame a la lista\n",
    "    lista_df_reviews.append(df)\n",
    "    \n",
    "    nombre_archivo = f'pq_review/review_{str(numero_archivo).zfill(2)}.parquet'\n",
    "    df.to_parquet(nombre_archivo, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_df_reviews)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvYelp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
