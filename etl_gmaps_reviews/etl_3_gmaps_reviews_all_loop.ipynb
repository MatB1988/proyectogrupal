{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modificar segun entorno local\n",
    "os.chdir( \"/Volumes/hd_mvf_datasets/henry_data\")\n",
    "\n",
    "# no modificar\n",
    "folder_data = \"1_external\"\n",
    "folder_pipeline = \"2_pipeline\"\n",
    "folder_output = \"3_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos una lista de los archivos en pipeline\n",
    "list_files_gmaps_state_norm = glob.glob(\n",
    "    os.path.join(folder_pipeline,\"*_norm.parquet\"))\n",
    "list_files_gmaps_state_norm.sort()\n",
    "\n",
    "# aplicamos read_parquet a todos los archivos de la lista\n",
    "# generamos una lista de archivos parquet\n",
    "list_dfs_gmaps_state_norm = [\n",
    "    pd.read_parquet(f) for f in list_files_gmaps_state_norm\n",
    "    ]\n",
    "\n",
    "# unimos los df de la lista de archivos parquet\n",
    "data_gmaps_reviews_norm = pd.concat(\n",
    "    list_dfs_gmaps_state_norm, ignore_index=True)\n",
    "\n",
    "# guardamos el archivo grande en output\n",
    "data_gmaps_reviews_norm.to_parquet(\n",
    "    os.path.join(folder_output, \"data_gmaps_reviews_norm.parquet\")\n",
    "    )\n",
    "data_gmaps_reviews_norm.info()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
